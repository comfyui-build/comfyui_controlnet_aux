# ComfyUIçš„ControlNetè¾…åŠ©é¢„å¤„ç†å™¨
![](./examples/example_mesh_graphormer.png)
å³æ’å³ç”¨çš„[ComfyUI](https://github.com/comfyanonymous/ComfyUI)èŠ‚ç‚¹é›†ï¼Œç”¨äºåˆ¶ä½œ[ControlNet](https://github.com/lllyasviel/ControlNet/)æç¤ºå›¾åƒ

ä»£ç æ˜¯ä»https://github.com/lllyasviel/ControlNet/tree/main/annotatorä¸­çš„ç›¸åº”æ–‡ä»¶å¤¹å¤åˆ¶ç²˜è´´çš„ï¼Œå¹¶è¿æ¥åˆ°[ğŸ¤— Hub](https://huggingface.co/lllyasviel/Annotators)ã€‚

æ‰€æœ‰ä¿¡ç”¨å’Œç‰ˆæƒå½’https://github.com/lllyasvielæ‰€æœ‰ã€‚

# Marigold
æŸ¥çœ‹Marigoldæ·±åº¦ä¼°è®¡å™¨ï¼Œå®ƒå¯ä»¥ä»é«˜åˆ†è¾¨ç‡é™æ€å›¾åƒç”Ÿæˆéå¸¸è¯¦ç»†å’Œæ¸…æ™°çš„æ·±åº¦å›¾ã€‚å®ƒåˆ›å»ºçš„ç½‘æ ¼ç”šè‡³å¯ä»¥3Dæ‰“å°ã€‚ç”±äºæ‰©æ•£å™¨çš„åŸå› ï¼Œå®ƒæ— æ³•åœ¨æ­¤æ‰©å±•ä¸­å®ç°ï¼Œä½†Kijaiæœ‰ä¸€ä¸ªComfyå®ç°
https://github.com/kijai/ComfyUI-Marigold

![](./examples/example_marigold_flat.jpg)
![](./examples/example_marigold.png)

# æ›´æ–°
å‰å¾€[æ›´æ–°é¡µé¢](./UPDATES.md)ä»¥å…³æ³¨æ›´æ–°

# å®‰è£…ï¼š
## ä½¿ç”¨ComfyUI Managerï¼ˆæ¨èï¼‰ï¼š
å®‰è£…[ComfyUI Manager](https://github.com/ltdrdata/ComfyUI-Manager)å¹¶æŒ‰ç…§é‚£é‡Œçš„æ­¥éª¤å®‰è£…æ­¤ä»“åº“ã€‚

## æ›¿ä»£æ–¹æ¡ˆï¼š
å¦‚æœä½ åœ¨Linuxä¸Šè¿è¡Œï¼Œæˆ–è€…åœ¨Windowsä¸Šçš„éç®¡ç†å‘˜è´¦æˆ·ï¼Œä½ éœ€è¦ç¡®ä¿`/ComfyUI/custom_nodes`å’Œ`comfyui_controlnet_aux`å…·æœ‰å†™æƒé™ã€‚

ç°åœ¨æœ‰ä¸€ä¸ª**install.bat**ä½ å¯ä»¥è¿è¡Œæ¥å®‰è£…åˆ°ä¾¿æºè®¾å¤‡ï¼ˆå¦‚æœæ£€æµ‹åˆ°ï¼‰ã€‚å¦åˆ™ï¼Œå®ƒå°†é»˜è®¤å®‰è£…åˆ°ç³»ç»Ÿï¼Œå¹¶å‡è®¾ä½ å·²ç»æŒ‰ç…§ComfyUIçš„æ‰‹åŠ¨å®‰è£…æ­¥éª¤æ“ä½œã€‚

å¦‚æœä½ æ— æ³•è¿è¡Œ**install.bat**ï¼ˆä¾‹å¦‚ï¼Œä½ æ˜¯Linuxç”¨æˆ·ï¼‰ã€‚æ‰“å¼€CMD/Shellå¹¶æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
  - å¯¼èˆªåˆ°ä½ çš„`/ComfyUI/custom_nodes/`æ–‡ä»¶å¤¹
  - è¿è¡Œ`git clone https://github.com/Fannovel16/comfyui_controlnet_aux/`
  - å¯¼èˆªåˆ°ä½ çš„`comfyui_controlnet_aux`æ–‡ä»¶å¤¹
    - ä¾¿æº/venvï¼š
       - è¿è¡Œ`path/to/ComfUI/python_embeded/python.exe -s -m pip install -r requirements.txt`
	- ä½¿ç”¨ç³»ç»Ÿpython
	   - è¿è¡Œ`pip install -r requirements.txt`
  - å¯åŠ¨ComfyUI

# èŠ‚ç‚¹
è¯·æ³¨æ„ï¼Œæ­¤ä»“åº“ä»…æ”¯æŒåˆ¶ä½œæç¤ºå›¾åƒï¼ˆä¾‹å¦‚ç«æŸ´äººã€cannyè¾¹ç¼˜ç­‰ï¼‰çš„é¢„å¤„ç†å™¨ã€‚
é™¤äº†Inpaintä¹‹å¤–çš„æ‰€æœ‰é¢„å¤„ç†å™¨éƒ½é›†æˆåˆ°`AIO Aux Preprocessor`èŠ‚ç‚¹ä¸­ã€‚
æ­¤èŠ‚ç‚¹å…è®¸ä½ å¿«é€Ÿè·å–é¢„å¤„ç†å™¨ï¼Œä½†é¢„å¤„ç†å™¨è‡ªèº«çš„é˜ˆå€¼å‚æ•°æ— æ³•è®¾ç½®ã€‚
ä½ éœ€è¦ç›´æ¥ä½¿ç”¨å…¶èŠ‚ç‚¹æ¥è®¾ç½®é˜ˆå€¼ã€‚

# èŠ‚ç‚¹ï¼ˆå„éƒ¨åˆ†æ˜¯Comfyèœå•ä¸­çš„åˆ†ç±»ï¼‰
## çº¿æ¡æå–å™¨ - Line Extractors
| Preprocessor Node           | sd-webui-controlnet/other |          ControlNet/T2I-Adapter           |
|-----------------------------|---------------------------|-------------------------------------------|
| Binary Lines                | binary                    | control_scribble                          |
| Canny Edge                  | canny                     | control_v11p_sd15_canny <br> control_canny <br> t2iadapter_canny |
| HED Soft-Edge Lines         | hed                       | control_v11p_sd15_softedge <br> control_hed |
| Standard Lineart            | standard_lineart          | control_v11p_sd15_lineart                 |
| Realistic Lineart           | lineart (or `lineart_coarse` if `coarse` is enabled) | control_v11p_sd15_lineart |
| Anime Lineart               | lineart_anime             | control_v11p_sd15s2_lineart_anime         |
| Manga Lineart               | lineart_anime_denoise     | control_v11p_sd15s2_lineart_anime         |
| M-LSD Lines                 | mlsd                      | control_v11p_sd15_mlsd <br> control_mlsd  |
| PiDiNet Soft-Edge Lines     | pidinet                   | control_v11p_sd15_softedge <br> control_scribble |
| Scribble Lines              | scribble                  | control_v11p_sd15_scribble <br> control_scribble |
| Scribble XDoG Lines         | scribble_xdog             | control_v11p_sd15_scribble <br> control_scribble |
| Fake Scribble Lines         | scribble_hed              | control_v11p_sd15_scribble <br> control_scribble |
| TEED Soft-Edge Lines        | teed                      | [controlnet-sd-xl-1.0-softedge-dexined](https://huggingface.co/SargeZT/controlnet-sd-xl-1.0-softedge-dexined/blob/main/controlnet-sd-xl-1.0-softedge-dexined.safetensors) <br> control_v11p_sd15_softedge (Theoretically)
| Scribble PiDiNet Lines      | scribble_pidinet          | control_v11p_sd15_scribble <br> control_scribble |
| AnyLine Lineart             |                           | mistoLine_fp16.safetensors <br> mistoLine_rank256 <br> control_v11p_sd15s2_lineart_anime <br> control_v11p_sd15_lineart |

## æ³•çº¿å’Œæ·±åº¦ä¼°è®¡å™¨ - Normal and Depth Estimators
| Preprocessor Node           | sd-webui-controlnet/other |          ControlNet/T2I-Adapter           |
|-----------------------------|---------------------------|-------------------------------------------|
| MiDaS Depth Map           | (normal) depth            | control_v11f1p_sd15_depth <br> control_depth <br> t2iadapter_depth |
| LeReS Depth Map           | depth_leres               | control_v11f1p_sd15_depth <br> control_depth <br> t2iadapter_depth |
| Zoe Depth Map             | depth_zoe                 | control_v11f1p_sd15_depth <br> control_depth <br> t2iadapter_depth |
| MiDaS Normal Map          | normal_map                | control_normal                            |
| BAE Normal Map            | normal_bae                | control_v11p_sd15_normalbae               |
| MeshGraphormer Hand Refiner ([HandRefinder](https://github.com/wenquanlu/HandRefiner))  | depth_hand_refiner | [control_sd15_inpaint_depth_hand_fp16](https://huggingface.co/hr16/ControlNet-HandRefiner-pruned/blob/main/control_sd15_inpaint_depth_hand_fp16.safetensors) |
| Depth Anything            |  depth_anything           | [Depth-Anything](https://huggingface.co/spaces/LiheYoung/Depth-Anything/blob/main/checkpoints_controlnet/diffusion_pytorch_model.safetensors) |
| Zoe Depth Anything <br> (Basically Zoe but the encoder is replaced with DepthAnything)       | depth_anything | [Depth-Anything](https://huggingface.co/spaces/LiheYoung/Depth-Anything/blob/main/checkpoints_controlnet/diffusion_pytorch_model.safetensors) |
| Normal DSINE              |                           | control_normal/control_v11p_sd15_normalbae |
| Metric3D Depth            |                           | control_v11f1p_sd15_depth <br> control_depth <br> t2iadapter_depth |
| Metric3D Normal           |                           | control_v11p_sd15_normalbae |
| Depth Anything V2         |                           | [Depth-Anything](https://huggingface.co/spaces/LiheYoung/Depth-Anything/blob/main/checkpoints_controlnet/diffusion_pytorch_model.safetensors) |

## é¢éƒ¨å’Œå§¿æ€ä¼°è®¡å™¨ - Faces and Poses Estimators
| Preprocessor Node           | sd-webui-controlnet/other |          ControlNet/T2I-Adapter           |
|-----------------------------|---------------------------|-------------------------------------------|
| DWPose Estimator                 | dw_openpose_full          | control_v11p_sd15_openpose <br> control_openpose <br> t2iadapter_openpose |
| OpenPose Estimator               | openpose (detect_body) <br> openpose_hand (detect_body + detect_hand) <br> openpose_faceonly (detect_face) <br> openpose_full (detect_hand + detect_body + detect_face)    | control_v11p_sd15_openpose <br> control_openpose <br> t2iadapter_openpose |
| MediaPipe Face Mesh         | mediapipe_face            | controlnet_sd21_laion_face_v2             | 
| Animal Estimator                 | animal_openpose           | [control_sd15_animal_openpose_fp16](https://huggingface.co/huchenlei/animal_openpose/blob/main/control_sd15_animal_openpose_fp16.pth) |

## å…‰æµä¼°è®¡å™¨ - Optical Flow Estimators
| Preprocessor Node           | sd-webui-controlnet/other |          ControlNet/T2I-Adapter           |
|-----------------------------|---------------------------|-------------------------------------------|
| Unimatch Optical Flow       |                           | [DragNUWA](https://github.com/ProjectNUWA/DragNUWA) |

### å¦‚ä½•è·å–OpenPoseæ ¼å¼çš„JSONï¼Ÿ
#### ç”¨æˆ·ç«¯
æ­¤å·¥ä½œæµç¨‹å°†å›¾åƒä¿å­˜åˆ°ComfyUIçš„è¾“å‡ºæ–‡ä»¶å¤¹ï¼ˆä¸è¾“å‡ºå›¾åƒçš„åŒä¸€ä½ç½®ï¼‰ã€‚å¦‚æœä½ è¿˜æ²¡æœ‰æ‰¾åˆ°`Save Pose Keypoints`èŠ‚ç‚¹ï¼Œè¯·æ›´æ–°æ­¤æ‰©å±•
![](./examples/example_save_kps.png)

#### å¼€å‘è€…ç«¯
ä¸€ä¸ªä¸IMAGEæ‰¹æ¬¡ä¸­æ¯ä¸ªå¸§å¯¹åº”çš„[OpenPoseæ ¼å¼JSON](https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/02_output.md#json-output-format)æ•°ç»„å¯ä»¥é€šè¿‡åœ¨UIä¸Šä½¿ç”¨`app.nodeOutputs`æˆ–åœ¨`/history` APIç«¯ç‚¹ä¸Šä½¿ç”¨DWPoseå’ŒOpenPoseè·å–ã€‚AnimalPoseçš„JSONè¾“å‡ºä½¿ç”¨ä¸OpenPose JSONç±»ä¼¼çš„æ ¼å¼ï¼š
```
[
    {
        "version": "ap10k",
        "animals": [
            [[x1, y1, 1], [x2, y2, 1],..., [x17, y17, 1]],
            [[x1, y1, 1], [x2, y2, 1],..., [x17, y17, 1]],
            ...
        ],
        "canvas_height": 512,
        "canvas_width": 768
    },
    ...
]
```

å¯¹äºæ‰©å±•å¼€å‘è€…ï¼ˆä¾‹å¦‚Openposeç¼–è¾‘å™¨ï¼‰ï¼š
```js
const poseNodes = app.graph._nodes.filter(node => ["OpenposePreprocessor", "DWPreprocessor", "AnimalPosePreprocessor"].includes(node.type))
for (const poseNode of poseNodes) {
    const openposeResults = JSON.parse(app.nodeOutputs[poseNode.id].openpose_json[0])
    console.log(openposeResults) //An array containing Openpose JSON for each frame
}
```

å¯¹äºAPIç”¨æˆ·ï¼š
Javascript
```js
import fetch from "node-fetch" //Remember to add "type": "module" to "package.json"
async function main() {
    const promptId = '792c1905-ecfe-41f4-8114-83e6a4a09a9f' //Too lazy to POST /queue
    let history = await fetch(`http://127.0.0.1:8188/history/${promptId}`).then(re => re.json())
    history = history[promptId]
    const nodeOutputs = Object.values(history.outputs).filter(output => output.openpose_json)
    for (const nodeOutput of nodeOutputs) {
        const openposeResults = JSON.parse(nodeOutput.openpose_json[0])
        console.log(openposeResults) //An array containing Openpose JSON for each frame
    }
}
main()
```

Python
```py
import json, urllib.request

server_address = "127.0.0.1:8188"
prompt_id = '' #Too lazy to POST /queue

def get_history(prompt_id):
    with urllib.request.urlopen("http://{}/history/{}".format(server_address, prompt_id)) as response:
        return json.loads(response.read())

history = get_history(prompt_id)[prompt_id]
for o in history['outputs']:
    for node_id in history['outputs']:
        node_output = history['outputs'][node_id]
        if 'openpose_json' in node_output:
            print(json.loads(node_output['openpose_json'][0])) #An list containing Openpose JSON for each frame
```
## è¯­ä¹‰åˆ†å‰² - Semantic Segmentation
| Preprocessor Node           | sd-webui-controlnet/other |          ControlNet/T2I-Adapter           |
|-----------------------------|---------------------------|-------------------------------------------|
| OneFormer ADE20K Segmentor  | oneformer_ade20k          | control_v11p_sd15_seg                     |
| OneFormer COCO Segmentor    | oneformer_coco            | control_v11p_sd15_seg                     |
| UniFormer Segmentor         | segmentation              |control_sd15_seg <br> control_v11p_sd15_seg|

## T2IAdapter-only
| Preprocessor Node           | sd-webui-controlnet/other |          ControlNet/T2I-Adapter           |
|-----------------------------|---------------------------|-------------------------------------------|
| Color Pallete               | color                     | t2iadapter_color                          |
| Content Shuffle             | shuffle                   | t2iadapter_style                          |

## ## é‡æ–°ç€è‰² - Recolor
| Preprocessor Node           | sd-webui-controlnet/other |          ControlNet/T2I-Adapter           |
|-----------------------------|---------------------------|-------------------------------------------|
| Image Luminance             | recolor_luminance         | [ioclab_sd15_recolor](https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/ioclab_sd15_recolor.safetensors) <br> [sai_xl_recolor_256lora](https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/sai_xl_recolor_256lora.safetensors) <br> [bdsqlsz_controlllite_xl_recolor_luminance](https://huggingface.co/bdsqlsz/qinglong_controlnet-lllite/resolve/main/bdsqlsz_controlllite_xl_recolor_luminance.safetensors) |
| Image Intensity             | recolor_intensity         | Idk. Maybe same as above? |

# ç¤ºä¾‹
> ä¸€å¼ å›¾ç‰‡èƒœè¿‡åƒè¨€ä¸‡è¯­

ä»¥ä¸‹å¤§å¤šæ•°ç¤ºä¾‹çš„åŠŸåŠ³å½’äºhttps://huggingface.co/thibaud/controlnet-sd21ã€‚ä½ å¯ä»¥ä»æœ¬ä»“åº“çš„é¢„å¤„ç†å™¨èŠ‚ç‚¹è·å¾—ç±»ä¼¼çš„ç»“æœã€‚
## çº¿æ¡æå–å™¨
### Cannyè¾¹ç¼˜ - Canny Edge
![](https://huggingface.co/thibaud/controlnet-sd21/resolve/main/example_canny.png)
### HEDçº¿æ¡ - HED Lines
![](https://huggingface.co/thibaud/controlnet-sd21/resolve/main/example_hed.png)
### çœŸå®æ„Ÿçº¿æ¡è‰ºæœ¯ - Realistic Lineart
![](https://huggingface.co/thibaud/controlnet-sd21/resolve/main/example_lineart.png)
### æ¶‚é¸¦/å‡æ¶‚é¸¦ - Scribble/Fake Scribble
![](https://huggingface.co/thibaud/controlnet-sd21/resolve/main/example_scribble.png)
### TEEDè½¯è¾¹ç¼˜çº¿æ¡ - TEED Soft-Edge Lines
![](./examples/example_teed.png)
### ä»»æ„çº¿æ¡è‰ºæœ¯ - Anyline Lineart
![](./examples/example_anyline.png)

## æ³•çº¿å’Œæ·±åº¦å›¾
### æ·±åº¦ï¼ˆä¸çŸ¥é“ä»–ä»¬ä½¿ç”¨çš„é¢„å¤„ç†å™¨ï¼‰ - Depth (idk the preprocessor they use)
![](https://huggingface.co/thibaud/controlnet-sd21/resolve/main/example_depth.png)
## Zoe - æ·±åº¦å›¾ - Zoe - Depth Map
![](https://huggingface.co/thibaud/controlnet-sd21/resolve/main/example_zoedepth.png)
## BAE - æ³•çº¿å›¾ - BAE - Normal Map
![](https://huggingface.co/thibaud/controlnet-sd21/resolve/main/example_normalbae.png)
## MeshGraphormer
![](./examples/example_mesh_graphormer.png)
## Depth Anything & Zoe Depth Anything
![](./examples/example_depth_anything.png)
## DSINE
![](./examples/example_dsine.png)
## Metric3D
![](./examples/example_metric3d.png)
## Depth Anything V2
![](./examples/example_depth_anything_v2.png)

## è„¸éƒ¨å’Œå§¿åŠ¿
### OpenPose
![](https://huggingface.co/thibaud/controlnet-sd21/resolve/main/example_openpose.png)
![](https://huggingface.co/thibaud/controlnet-sd21/resolve/main/example_openposev2.png)

### Animal Pose (AP-10K)
![](./examples/example_animal_pose.png)

### DensePose
![](./examples/example_densepose.png)

## è¯­ä¹‰åˆ†å‰²
### OneFormer ADE20K Segmentor
![](https://huggingface.co/thibaud/controlnet-sd21/resolve/main/example_ade20k.png)

### Anime Face Segmentor
![](./examples/example_anime_face_segmentor.png)

## T2IAdapter-only
### Color Pallete for T2I-Adapter
![](https://huggingface.co/thibaud/controlnet-sd21/resolve/main/example_color.png)

## å…‰æµ - Optical Flow
### Unimatch
![](./examples/example_unimatch.png)

## é‡æ–°ç€è‰² - Recolor
![](./examples/example_recolor.png)

# æµ‹è¯•å·¥ä½œæµç¨‹
https://github.com/Fannovel16/comfyui_controlnet_aux/blob/master/tests/test_cn_aux_full.json
![](https://github.com/Fannovel16/comfyui_controlnet_aux/blob/master/tests/pose.png?raw=true)

# é—®ç­”ï¼š
## ä¸ºä»€ä¹ˆåœ¨æˆ‘å®‰è£…äº†è¿™ä¸ªä»“åº“åï¼Œæœ‰äº›èŠ‚ç‚¹æ²¡æœ‰å‡ºç°ï¼Ÿ

è¿™ä¸ªä»“åº“æœ‰ä¸€ä¸ªæ–°çš„æœºåˆ¶ï¼Œä¼šè·³è¿‡ä»»ä½•æ— æ³•å¯¼å…¥çš„è‡ªå®šä¹‰èŠ‚ç‚¹ã€‚å¦‚æœä½ é‡åˆ°è¿™ç§æƒ…å†µï¼Œè¯·åœ¨[Issues tab](https://github.com/Fannovel16/comfyui_controlnet_aux/issues)ä¸Šåˆ›å»ºä¸€ä¸ªissueï¼Œå¹¶é™„ä¸Šå‘½ä»¤è¡Œä¸­çš„æ—¥å¿—ã€‚

## DWPose/AnimalPose åªä½¿ç”¨CPUï¼Œæ‰€ä»¥é€Ÿåº¦å¾ˆæ…¢ã€‚æˆ‘æ€æ ·æ‰èƒ½è®©å®ƒä½¿ç”¨GPUï¼Ÿ
æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥åŠ é€ŸDWPoseï¼šä½¿ç”¨TorchScriptæ£€æŸ¥ç‚¹ï¼ˆ.torchscript.ptï¼‰æˆ–ONNXRuntimeï¼ˆ.onnxï¼‰ã€‚TorchScriptæ–¹å¼æ¯”ONNXRuntimeç¨æ…¢ï¼Œä½†ä¸éœ€è¦ä»»ä½•é¢å¤–çš„åº“ï¼Œå¹¶ä¸”ä»ç„¶æ¯”CPUå¿«å¾ˆå¤šã€‚

ä¸€ä¸ªtorchscriptçš„è¾¹ç•Œæ¡†æ£€æµ‹å™¨å¯ä»¥ä¸ä¸€ä¸ªonnxçš„å§¿æ€ä¼°è®¡å™¨å…¼å®¹ï¼Œåä¹‹äº¦ç„¶ã€‚
### TorchScript
æ ¹æ®è¿™å¼ å›¾ç‰‡è®¾ç½®`bbox_detector`å’Œ`pose_estimator`ã€‚å¦‚æœè¾“å…¥å›¾åƒç†æƒ³ï¼Œä½ å¯ä»¥å°è¯•å…¶ä»–ä»¥`.torchscript.pt`ç»“å°¾çš„è¾¹ç•Œæ¡†æ£€æµ‹å™¨æ¥å‡å°‘è¾¹ç•Œæ¡†æ£€æµ‹æ—¶é—´ã€‚
![](./examples/example_torchscript.png)
### ONNXRuntime
å¦‚æœonnxruntimeå®‰è£…æˆåŠŸï¼Œå¹¶ä¸”ä½¿ç”¨çš„æ£€æŸ¥ç‚¹ä»¥`.onnx`ç»“å°¾ï¼Œå®ƒå°†æ›¿æ¢é»˜è®¤çš„cv2åç«¯ä»¥åˆ©ç”¨GPUã€‚è¯·æ³¨æ„ï¼Œå¦‚æœä½ ä½¿ç”¨çš„æ˜¯NVidiaæ˜¾å¡ï¼Œè¿™ç§æ–¹æ³•ç›®å‰åªèƒ½åœ¨CUDA 11.8ï¼ˆComfyUI_windows_portable_nvidia_cu118_or_cpu.7zï¼‰ä¸Šå·¥ä½œï¼Œé™¤éä½ è‡ªå·±ç¼–è¯‘onnxruntimeã€‚

1. äº†è§£ä½ çš„onnxruntimeæ„å»ºï¼š
   - **NVidia CUDA 11.x æˆ–ä»¥ä¸‹/AMD GPU**: `onnxruntime-gpu`
   - **NVidia CUDA 12.x**: `onnxruntime-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/`
   - **DirectML**: `onnxruntime-directml`
   - **OpenVINO**: `onnxruntime-openvino`

   è¯·æ³¨æ„ï¼Œå¦‚æœä½ æ˜¯ç¬¬ä¸€æ¬¡ä½¿ç”¨ComfyUIï¼Œè¯·å…ˆæµ‹è¯•å®ƒæ˜¯å¦èƒ½åœ¨ä½ çš„è®¾å¤‡ä¸Šè¿è¡Œï¼Œç„¶åå†è¿›è¡Œä¸‹ä¸€æ­¥ã€‚

2. å°†å…¶æ·»åŠ åˆ°`requirements.txt`ä¸­

3. è¿è¡Œ`install.bat`æˆ–å®‰è£…éƒ¨åˆ†æåˆ°çš„pipå‘½ä»¤

![](./examples/example_onnx.png)

# Assets files of preprocessors
* anime_face_segment:  [bdsqlsz/qinglong_controlnet-lllite/Annotators/UNet.pth](https://huggingface.co/bdsqlsz/qinglong_controlnet-lllite/blob/main/Annotators/UNet.pth), [anime-seg/isnetis.ckpt](https://huggingface.co/skytnt/anime-seg/blob/main/isnetis.ckpt)
* densepose:  [LayerNorm/DensePose-TorchScript-with-hint-image/densepose_r50_fpn_dl.torchscript](https://huggingface.co/LayerNorm/DensePose-TorchScript-with-hint-image/blob/main/densepose_r50_fpn_dl.torchscript)
* dwpose:  
* * bbox_detector: Either [yzd-v/DWPose/yolox_l.onnx](https://huggingface.co/yzd-v/DWPose/blob/main/yolox_l.onnx), [hr16/yolox-onnx/yolox_l.torchscript.pt](https://huggingface.co/hr16/yolox-onnx/blob/main/yolox_l.torchscript.pt), [hr16/yolo-nas-fp16/yolo_nas_l_fp16.onnx](https://huggingface.co/hr16/yolo-nas-fp16/blob/main/yolo_nas_l_fp16.onnx), [hr16/yolo-nas-fp16/yolo_nas_m_fp16.onnx](https://huggingface.co/hr16/yolo-nas-fp16/blob/main/yolo_nas_m_fp16.onnx), [hr16/yolo-nas-fp16/yolo_nas_s_fp16.onnx](https://huggingface.co/hr16/yolo-nas-fp16/blob/main/yolo_nas_s_fp16.onnx)
* * pose_estimator: Either [hr16/DWPose-TorchScript-BatchSize5/dw-ll_ucoco_384_bs5.torchscript.pt](https://huggingface.co/hr16/DWPose-TorchScript-BatchSize5/blob/main/dw-ll_ucoco_384_bs5.torchscript.pt), [yzd-v/DWPose/dw-ll_ucoco_384.onnx](https://huggingface.co/yzd-v/DWPose/blob/main/dw-ll_ucoco_384.onnx)
* animal_pose (ap10k):
* * bbox_detector: Either [yzd-v/DWPose/yolox_l.onnx](https://huggingface.co/yzd-v/DWPose/blob/main/yolox_l.onnx), [hr16/yolox-onnx/yolox_l.torchscript.pt](https://huggingface.co/hr16/yolox-onnx/blob/main/yolox_l.torchscript.pt), [hr16/yolo-nas-fp16/yolo_nas_l_fp16.onnx](https://huggingface.co/hr16/yolo-nas-fp16/blob/main/yolo_nas_l_fp16.onnx), [hr16/yolo-nas-fp16/yolo_nas_m_fp16.onnx](https://huggingface.co/hr16/yolo-nas-fp16/blob/main/yolo_nas_m_fp16.onnx), [hr16/yolo-nas-fp16/yolo_nas_s_fp16.onnx](https://huggingface.co/hr16/yolo-nas-fp16/blob/main/yolo_nas_s_fp16.onnx)
* * pose_estimator: Either [hr16/DWPose-TorchScript-BatchSize5/rtmpose-m_ap10k_256_bs5.torchscript.pt](https://huggingface.co/hr16/DWPose-TorchScript-BatchSize5/blob/main/rtmpose-m_ap10k_256_bs5.torchscript.pt), [hr16/UnJIT-DWPose/rtmpose-m_ap10k_256.onnx](https://huggingface.co/hr16/UnJIT-DWPose/blob/main/rtmpose-m_ap10k_256.onnx)
* hed:  [lllyasviel/Annotators/ControlNetHED.pth](https://huggingface.co/lllyasviel/Annotators/blob/main/ControlNetHED.pth)
* leres:  [lllyasviel/Annotators/res101.pth](https://huggingface.co/lllyasviel/Annotators/blob/main/res101.pth), [lllyasviel/Annotators/latest_net_G.pth](https://huggingface.co/lllyasviel/Annotators/blob/main/latest_net_G.pth)
* lineart:  [lllyasviel/Annotators/sk_model.pth](https://huggingface.co/lllyasviel/Annotators/blob/main/sk_model.pth), [lllyasviel/Annotators/sk_model2.pth](https://huggingface.co/lllyasviel/Annotators/blob/main/sk_model2.pth)
* lineart_anime:  [lllyasviel/Annotators/netG.pth](https://huggingface.co/lllyasviel/Annotators/blob/main/netG.pth)
* manga_line:  [lllyasviel/Annotators/erika.pth](https://huggingface.co/lllyasviel/Annotators/blob/main/erika.pth)
* mesh_graphormer:  [hr16/ControlNet-HandRefiner-pruned/graphormer_hand_state_dict.bin](https://huggingface.co/hr16/ControlNet-HandRefiner-pruned/blob/main/graphormer_hand_state_dict.bin), [hr16/ControlNet-HandRefiner-pruned/hrnetv2_w64_imagenet_pretrained.pth](https://huggingface.co/hr16/ControlNet-HandRefiner-pruned/blob/main/hrnetv2_w64_imagenet_pretrained.pth)
* midas:  [lllyasviel/Annotators/dpt_hybrid-midas-501f0c75.pt](https://huggingface.co/lllyasviel/Annotators/blob/main/dpt_hybrid-midas-501f0c75.pt)
* mlsd:  [lllyasviel/Annotators/mlsd_large_512_fp32.pth](https://huggingface.co/lllyasviel/Annotators/blob/main/mlsd_large_512_fp32.pth)
* normalbae:  [lllyasviel/Annotators/scannet.pt](https://huggingface.co/lllyasviel/Annotators/blob/main/scannet.pt)
* oneformer:  [lllyasviel/Annotators/250_16_swin_l_oneformer_ade20k_160k.pth](https://huggingface.co/lllyasviel/Annotators/blob/main/250_16_swin_l_oneformer_ade20k_160k.pth)
* open_pose:  [lllyasviel/Annotators/body_pose_model.pth](https://huggingface.co/lllyasviel/Annotators/blob/main/body_pose_model.pth), [lllyasviel/Annotators/hand_pose_model.pth](https://huggingface.co/lllyasviel/Annotators/blob/main/hand_pose_model.pth), [lllyasviel/Annotators/facenet.pth](https://huggingface.co/lllyasviel/Annotators/blob/main/facenet.pth)
* pidi:  [lllyasviel/Annotators/table5_pidinet.pth](https://huggingface.co/lllyasviel/Annotators/blob/main/table5_pidinet.pth)
* sam:  [dhkim2810/MobileSAM/mobile_sam.pt](https://huggingface.co/dhkim2810/MobileSAM/blob/main/mobile_sam.pt)
* uniformer:  [lllyasviel/Annotators/upernet_global_small.pth](https://huggingface.co/lllyasviel/Annotators/blob/main/upernet_global_small.pth)
* zoe:  [lllyasviel/Annotators/ZoeD_M12_N.pt](https://huggingface.co/lllyasviel/Annotators/blob/main/ZoeD_M12_N.pt)
* teed:  [bdsqlsz/qinglong_controlnet-lllite/7_model.pth](https://huggingface.co/bdsqlsz/qinglong_controlnet-lllite/blob/main/Annotators/7_model.pth)
* depth_anything: Either [LiheYoung/Depth-Anything/checkpoints/depth_anything_vitl14.pth](https://huggingface.co/spaces/LiheYoung/Depth-Anything/blob/main/checkpoints/depth_anything_vitl14.pth), [LiheYoung/Depth-Anything/checkpoints/depth_anything_vitb14.pth](https://huggingface.co/spaces/LiheYoung/Depth-Anything/blob/main/checkpoints/depth_anything_vitb14.pth) or [LiheYoung/Depth-Anything/checkpoints/depth_anything_vits14.pth](https://huggingface.co/spaces/LiheYoung/Depth-Anything/blob/main/checkpoints/depth_anything_vits14.pth)
* diffusion_edge: Either [hr16/Diffusion-Edge/diffusion_edge_indoor.pt](https://huggingface.co/hr16/Diffusion-Edge/blob/main/diffusion_edge_indoor.pt), [hr16/Diffusion-Edge/diffusion_edge_urban.pt](https://huggingface.co/hr16/Diffusion-Edge/blob/main/diffusion_edge_urban.pt) or [hr16/Diffusion-Edge/diffusion_edge_natrual.pt](https://huggingface.co/hr16/Diffusion-Edge/blob/main/diffusion_edge_natrual.pt)
* unimatch: Either [hr16/Unimatch/gmflow-scale2-regrefine6-mixdata.pth](https://huggingface.co/hr16/Unimatch/blob/main/gmflow-scale2-regrefine6-mixdata.pth), [hr16/Unimatch/gmflow-scale2-mixdata.pth](https://huggingface.co/hr16/Unimatch/blob/main/gmflow-scale2-mixdata.pth) or [hr16/Unimatch/gmflow-scale1-mixdata.pth](https://huggingface.co/hr16/Unimatch/blob/main/gmflow-scale1-mixdata.pth)
* zoe_depth_anything: Either [LiheYoung/Depth-Anything/checkpoints_metric_depth/depth_anything_metric_depth_indoor.pt](https://huggingface.co/spaces/LiheYoung/Depth-Anything/blob/main/checkpoints_metric_depth/depth_anything_metric_depth_indoor.pt) or [LiheYoung/Depth-Anything/checkpoints_metric_depth/depth_anything_metric_depth_outdoor.pt](https://huggingface.co/spaces/LiheYoung/Depth-Anything/blob/main/checkpoints_metric_depth/depth_anything_metric_depth_outdoor.pt)
# 1500 Stars ğŸ˜„
<a href="https://star-history.com/#Fannovel16/comfyui_controlnet_aux&Date">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=Fannovel16/comfyui_controlnet_aux&type=Date&theme=dark" />
    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=Fannovel16/comfyui_controlnet_aux&type=Date" />
    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=Fannovel16/comfyui_controlnet_aux&type=Date" />
  </picture>
</a>

æ„Ÿè°¢å¤§å®¶çš„æ”¯æŒã€‚æˆ‘ä»æœªæƒ³è¿‡æ˜Ÿæ˜Ÿçš„å›¾è¡¨ä¼šæ˜¯çº¿æ€§çš„ï¼Œå“ˆå“ˆã€‚
